# Project: Systematic Literature Screening Automation Benchmark: LLM + Active Learning

- This repository contains experiments and tools for comparing Active Learning (AL) and Large Language Model (LLM) approaches to systematic literature review (SLR) screening. The project benchmarks multiple models (LLMs, traditional ML, and AL strategies) on biomedical datasets, analyzing trade-offs between recall, precision, and efficiency.

## Repository Structure

- experimental_env/: Contains the testing environment used during experimentation, including:
- prod_env/: A production-ready screening tool, adapted from the experimental environment for easier deployment and use.
- Analysis/ Includes  An SQL database storing results of all experiments and a Jupyter notebook that explores, analyzes, and visualizes experimental results, highlighting key insights.
- PDF: Academic paper based on the this work submitted to Humboldt University on SoSe'25
