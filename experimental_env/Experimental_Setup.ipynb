{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc0f1b9",
   "metadata": {},
   "source": [
    "## Experimental Environment and Design\n",
    "\n",
    "This notebook demonstrates how the experimental environment can be used and provides examples.\n",
    "The notebook will mainly focus on:\n",
    "- Creating `prompt` templates\n",
    "- Creating and running `experiment`\n",
    "\n",
    "\n",
    "### Requirements:\n",
    "For the environment to be used the following is required:\n",
    "- All libraries found in 'requirements.txt' need to be present\n",
    "- The dataset needs to have 'record_id' as the id column and 'label' (0 or 1). At least one of the following columns need to be included:\n",
    "* A column with 'openalex' in the name, which contains a link to the article on OpenAlex platform\n",
    "\n",
    "OR\n",
    "\n",
    "* 'title' and/or 'abstract' and/or 'keywords'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29412159",
   "metadata": {},
   "source": [
    "##### Key Parameters Embedded in The Environment\n",
    "\n",
    "- The cut-off value for the classic models is tau= 0.5\n",
    "- The stopping criterion for active learning is: 5% negatives of the total dataset found in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc874186",
   "metadata": {},
   "source": [
    "### `prompt`\n",
    "`prompt` is used to allow the user to create the template of their desired prompt. It follows RAG style prompts, as it provides two parts of the prompt, 'augmentation' (i.e. providing context) and 'prediction'. However, it is not necessary to follow RAG methodology, e.g. by not using the augmentation part. \n",
    "\n",
    "In both phases (augmentation and prediciton) a list of items will be presented. However, the specific datapoints will not be given by the user directly in the prompt, rather they will be handled by the environment, based on the datasets the user would provide. Therefore, at this stage, the user simply specifices the pattern in which each item in the augmentation and prediction list should look like. \n",
    "The pattern will be a combination of text provided by the user and placeholders for specific data extracted from the data points. For the latter, the following placeholders are supported\n",
    "- `{record_id}`: The ID of the article in the dataset\n",
    "- `{label_token}`: Since the dataset contains the label as 0 or 1, this allows the user to present a different labeling scheme by using the labels provided in the prompt attribute `positive_token` and `negative_token` instead. E.g. if the datapoint in question is positive, this place holder will be replaced by the value provided in `positive_token`.\n",
    "- `{title}`\n",
    "- `{abstract}`\n",
    "- `{keywords}`\n",
    "\n",
    "*Note*: If a list is provided, please place '{}' where the list should appear in the 'augmentation' or 'prediction' part.\n",
    "\n",
    "\n",
    "#### `positive_token` and `negative_token`:\n",
    "If a labeling scheme is used in the prompt, this allows the user to define it and would also be necessary for the environment to read the response of the LLM correctly.\n",
    "\n",
    "#### prediction_method\n",
    "- `Methods.ID`: Model returns a list of IDs for relevant items.\n",
    "- `Methods.TOKEN`: Model returns a token (e.g., '<POSITIVE>' or '<NEGATIVE>') for each item.\n",
    "- `Methods.ID_TOKEN`: Model returns both an ID and a token (e.g. '{134B:<POSITIVE>}') for each item.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import Prompt, Methods\n",
    "\n",
    "# ID method\n",
    "prompt_id = Prompt(\n",
    "    augmentation='List relevant items: {}',\n",
    "    augmentation_item_pattern='{\"ID\":\"{record_id}\", content: {title} {abstract} }',\n",
    "    prediction='Provide me the IDs of the relevant items here: {}',\n",
    "    prediction_item_pattern='{\"ID\":\"{record_id}\", content: {title} {abstract} }',\n",
    "    prediction_method=Methods.ID\n",
    ")\n",
    "\n",
    "# TOKEN method\n",
    "prompt_token = Prompt(\n",
    "    augmentation='Classify each item as <POSITIVE> or <NEGATIVE>: {}',\n",
    "    augmentation_item_pattern='$$$ {title} {abstract} , STATUS={label_token}  $$$',\n",
    "    prediction='Predict the STATUS: {}',\n",
    "    prediction_item_pattern='$$${title} {abstract}, STATUS= ',\n",
    "    positive_token='<POSITIVE>', \n",
    "    negative_token='<NEGATIVE>',\n",
    "    prediction_method=Methods.TOKEN\n",
    ")\n",
    "\n",
    "# ID_TOKEN method\n",
    "prompt_id_token = Prompt(\n",
    "    augmentation='List items and their status: {}',\n",
    "    augmentation_item_pattern='{\"ID\":\"{record_id}\", content: {title} {abstract}, \"STATUS\":\"{label_token}\" }',\n",
    "    prediction='Which are relevant? {}',\n",
    "    prediction_item_pattern='{\"ID\":\"{record_id}\", content: {title} {abstract} }',\n",
    "    positive_token='<POSITIVE>',\n",
    "    negative_token='<NEGATIVE>',\n",
    "    prediction_method=Methods.ID_TOKEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ca9f4",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "The experiment object is the object that acts as an interface between the user and the larger environment. For its initialization, it requires the dataset to be experimented on, passed either as shown here, however, a link to a direct file is also permissable. Then, the columns, which will be later used, have to also be defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import Experiment\n",
    "\n",
    "exp = Experiment('example_dataset.csv', columns=['title', 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73684894",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "The supported approaches are: 'active'/'active-learning' and 'zero-shot'/'few-shot'. However, the latter is defined in two steps, 1- the input has to contain 'shot' in the name, but weather it is actually zero or few shot is determined by the number of positive/negative class examples which will be determined later\n",
    "\n",
    "#### Model\n",
    "The following classical models are supported:\n",
    "- **Naive bayes**\n",
    "- **Random Forest**\n",
    "- **Logistic Regression**\n",
    "\n",
    "Simply write any part of the model name, and it should be detected\n",
    "\n",
    "The following LLMs are supported:\n",
    "- **HU LLM 1 and 3**, e.g. simply write 'HU-LLM3'\n",
    "- **Gemini**, e.g. 'gemini-2.0-flash' (follows google's naming scheme)\n",
    "- **Local models** reachable via OpenWebUI, e.g. 'OpenWebUI/qwen3:8b'\n",
    "- **Any model reached via huggingface's inference API**, e.g. 'hf-inference/meta-llama/Llama-3.3-70B-Instruct'. If you want to use a different provider simply chage the first part (i.e. replace 'hf-inference' via a different supported provider)\n",
    "\n",
    "*Note*: if you use a different LLM than HU, then you need to provide an api_key via api_key attribute of the environment object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic model\n",
    "model = 'bayes'\n",
    "# Google Gemini LLM\n",
    "model = 'gemini-2.0-flash'\n",
    "\n",
    "exp.model=model\n",
    "\n",
    "\n",
    "exp.api_key='your_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be10fe",
   "metadata": {},
   "source": [
    "## Full Example: Running an Experiment\n",
    "Below is a complete example using all the parameters above, including prompt, approach, and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import Experiment\n",
    "from prompt import Prompt, Methods\n",
    "\n",
    "# Define a prompt using TOKEN method\n",
    "prompt = Prompt(\n",
    "    augmentation='Classify each item as <POSITIVE> or <NEGATIVE>: {}',\n",
    "    augmentation_item_pattern='$$$ {title} {abstract} , STATUS={label_token}  $$$',\n",
    "    prediction='Predict the STATUS: {}',\n",
    "    prediction_item_pattern='$$${title} {abstract}, STATUS= ',\n",
    "    positive_token='<POSITIVE>',\n",
    "    negative_token='<NEGATIVE>',\n",
    "    prediction_method=Methods.TOKEN\n",
    ")\n",
    "\n",
    "# A link would work too, e.g. 'https://raw.githubusercontent.com/asreview/synergy-dataset/master/datasets/Cohen_2006/Nelson_2002_ids.csv'\n",
    "exp = Experiment('Nelson_2002_ids.csv', columns=['title', 'abstract'])\n",
    "exp.prompt = prompt\n",
    "\n",
    "exp.approach='active'\n",
    "# Here you define how many datapoint of the positive and negative class\n",
    "# will be used in the training/augmentation phase\n",
    "exp.set_initial_data(positives=5, negatives=1)\n",
    "# Here you define the batch settings, if applicable\n",
    "exp.set_batch_settings(max_train_batch_size=50, max_predict_batch_size=10, delay=1)\n",
    "exp.model = 'gemini-2.0-flash' \n",
    "exp.api_key='your_api_key'\n",
    "predictions = exp.run()\n",
    "labels = exp.labels\n",
    "exp.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apa_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
